# stt.py
import sounddevice as sd
import numpy as np
from faster_whisper import WhisperModel

model = WhisperModel("small", device="cuda", compute_type="float16")

# stt.py

# ... (×”×§×•×“ ×©×œ××¢×œ×” ×¢× ×”-import ×•×”-WhisperModel × ×©××¨ ××•×ª×• ×“×‘×¨) ...

def record_until_silence(samplerate=16000, silence_threshold=0.015, silence_duration=2.5, min_recording_duration=0.5):
    """
    ××§×œ×™×˜ ××•×“×™×• ×•××¤×¡×™×§ ×›×©×™×© ×©×ª×™×§×” ×××•×©×›×ª.
    - silence_threshold: ×”×•×¨×“× ×• ××¢×˜ (×œ-0.015) ×›×“×™ ×©×œ× ×™×—×©×•×‘ ×©××™×œ×™× ×—×œ×©×•×ª ×”×Ÿ "×©×§×˜".
    - silence_duration: ×”×•×¢×œ×” ×œ-2.5 ×©× ×™×•×ª, ×›×“×™ ×œ×ª×ª ×œ×š ×–××Ÿ ×œ×§×—×ª ××•×•×™×¨ ××• ×œ×—×©×•×‘ ×‘×××¦×¢ ×”××©×¤×˜.
    - min_recording_duration: ××—×™×™×‘ ×œ×¤×—×•×ª ×—×¦×™ ×©× ×™×™×” ×©×œ ×“×™×‘×•×¨.
    """
    chunk_size = int(samplerate * 0.1)  # ×¨×¡×™×¡×™× ×©×œ 100ms
    audio_chunks = []
    silent_chunks = 0
    required_silent = int(silence_duration / 0.1)
    
    is_speaking = False

    with sd.InputStream(samplerate=samplerate, channels=1, dtype='float32') as stream:
        print("ğŸ¤ ××—×›×” ×œ×“×™×‘×•×¨...")
        while True:
            chunk, _ = stream.read(chunk_size)
            rms = np.sqrt(np.mean(chunk**2))
            
            # ×× ×™×© ×§×•×œ ×©×—×–×§ ××”×¡×£
            if rms > silence_threshold:
                if not is_speaking:
                    print("ğŸ—£ï¸ ××–×”×” ×“×™×‘×•×¨, ××§×œ×™×˜...")
                is_speaking = True
                silent_chunks = 0  # ×××¤×¡ ××ª ××•× ×” ×”×©×ª×™×§×” ×›×™ ×“×™×‘×¨×ª
            else:
                # ×× ×× ×—× ×• ×›×‘×¨ ×‘×ª×•×š ×”×§×œ×˜×”, × ×ª×—×™×œ ×œ×¡×¤×•×¨ ××ª ×”×©×ª×™×§×”
                if is_speaking:
                    silent_chunks += 1
                
            if is_speaking:
                audio_chunks.append(chunk)
                
            # ×ª× ××™ ×¢×¦×™×¨×”: ×“×™×‘×¨× ×•, ×•×¢×›×©×™×• ×©×ª×§× ×• ×œ××©×š silence_duration ×©×œ× (2.5 ×©× ×™×•×ª ×¢×›×©×™×•)
            if is_speaking and silent_chunks >= required_silent:
                print("â³ ×”×”×§×œ×˜×” ×”×¡×ª×™×™××” (×©×ª×™×§×” ××¨×•×›×” ××“×™). ××¢×‘×“...")
                break

    # ×‘×“×™×§×” ×”×× ×”×”×§×œ×˜×” ×§×¦×¨×” ××“×™
    total_duration = (len(audio_chunks) - silent_chunks) * 0.1 # ×–××Ÿ × ×˜×• ×‘×œ×™ ×©×ª×™×§×ª ×”×¡×•×£
    if total_duration < min_recording_duration:
        return None  # ×§×¦×¨ ××“×™

    audio = np.concatenate(audio_chunks).flatten()
    return audio

# ... (×©××¨ ×”×¤×•× ×§×¦×™×” transcribe × ×©××¨×ª ××•×ª×• ×“×‘×¨) ...


def transcribe(audio: np.ndarray, language="he") -> str:
    # prompt ×¨××©×•× ×™ ××•× ×¢ ×”×¨×‘×” ××‘×¢×™×•×ª ×”"×”×–×™×”" ×©×œ ××™×œ×™× ×¨×™×§×•×ª ×›××• Thank you
    segments, _ = model.transcribe(
        audio, 
        language=language, 
        beam_size=5,
        condition_on_previous_text=False, # ××•× ×¢ ×—×–×¨×” ×¢×œ ××©×¤×˜×™× ×§×•×“××™×
        vad_filter=True, # ××¤×¢×™×œ ××¡× ×Ÿ ×§×•×œ ××•×‘× ×” ×©×œ Whisper ×©××ª×¢×œ× ××¨×¢×©×™×
        vad_parameters=dict(min_silence_duration_ms=500)
    )
    
    text = " ".join(s.text for s in segments).strip()
    return text
